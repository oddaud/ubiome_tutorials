{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Getting Started\n",
    "\n",
    "Welcome! In this notebook we will analyze our uBiome data using qiime. \n",
    "\n",
    "Where you should be right now:\n",
    "* ssh 'd into an EC2 instance of the qiime community AMI\n",
    "* Have copied the processed uBiome fastq files onto the EC2 instance\n",
    "* These files should be in a folder in the home directory called ubiome_data, and should be labelled with a unique sequence ID followed by \\_R1_.fastq and \\_R2\\_.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a mapping file\n",
    "\n",
    "You also need to create a mapping file for your samples. The qiime mapping file contains information about the sample IDs, barcodes, primers, and information about the samples that you chose to include (for example, sampling location, date, and species). These files must be in a very particular format, read about it [here](http://qiime.org/documentation/file_formats.html). For an example file, look [here](https://docs.google.com/spreadsheets/u/1/d/1FXHtTmvw1gM4oUMbRdwQIEOZJlhFGeMNUvZmuEFqpps/pubhtml?gid=0&single=true).\n",
    "\n",
    "Ubiome data is slightly different than most data that is inputted into qiime because it has already been demultiplexed and the primers trimmed, which means those two columns will remain blank in your mapping file. However, they still must be present. \n",
    "\n",
    "If you have not already, generate a mapping file for your samples. If it is easiest, [download this example](https://raw.githubusercontent.com/oddaud/ubiome_tutorials/master/map.txt) and edit the data in excel. This is most easily done in an excel spreadsheet, but the file must be saved as a tab seperated values (.tsv). Transfer this file to the AWS community server, and place it in the ubiomeSamples folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##DO NOT evaluate this cell! This command must be edited, copied, and run from yout LOCAL terminal. \n",
    "scp -i [location of your key] [path to your map.tsv] ubuntu@[public DNS]:/home/ubuntu/ubiomeSamples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now test to see if your mapping file is formated correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from os import chdir\n",
    "from IPython.display import FileLinks, FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#enter ubiomeSamples directory\n",
    "chdir('ubiomeSamples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ubiomeSamples\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors and/or warnings detected in mapping file.  Please check the log and html file for details.\r\n"
     ]
    }
   ],
   "source": [
    "!validate_mapping_file.py -o vmf-map/ -m ../map.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Path (<tt>vmf-map-bad</tt>) doesn't exist. It may still be in the process of being generated, or you may have the incorrect path."
      ],
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLinks('vmf-map-bad/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Merging paired end reads\n",
    "\n",
    " Next, we will merge paired end reads using the qiime script [multiple_join_paired_ends.py](http://qiime.org/scripts/multiple_join_paired_ends.html). To view the options for any qiime script (and most scripts in general), use the -h option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!multiple_join_paired_ends.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The V4 region is 254 bp generally, but may deviate by a few bp in some cases. If you examine the length of each read in either the R1 or R2 file, you will notice that they are each 150 bp long. With a little math, you will notice then that each V4 region is covered completely by a read from either end, with an overlap of 46 bp in the middle.\n",
    "\n",
    "To merge our paired end reads, specify inputs, outputs, and the parameters. The parameters must be passed in a seperate file because multiple_join_paired_ends.py calls a different function, join_paired_ends.py, and we need to use parameters other than the default for join_paired_ends.py. To create a params file, we will write a quick file with python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('join_paired_end_reads_params.txt', 'w+') as f:\n",
    "                        f.write('join_paired_ends:perc_max_diff    99')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to join the paired ends. Don't forget to wait until the process is finishes ([\\*] is gone) to move on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!multiple_join_paired_ends.py -i ./ -o merged -p join_paired_endreads_params.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At this point you have a file called \\*merged.fastq for each sample. Qiime requires one single fastq file with each read labelled with a sample ID to do future analyses. Time to merge again. \n",
    "\n",
    "This requires some work by hand. To begin, write out each file in alphabetical order, separated by commas but NO SPACES. You can use ls -l to see them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ubiomeSamples/merged\r\n"
     ]
    }
   ],
   "source": [
    "chdir('merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ubiomeSamples/merged\n",
      "total 462360\n",
      "-rw-rw-r-- 1 ubuntu ubuntu      1886 Feb 26 22:13 log_20170226221319.txt\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  82620538 Feb 26 22:13 ssr_8730_R1_.merged.fastq\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 204009871 Feb 26 22:13 ssr_8737_R1_.merged.fastq\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 186813427 Feb 26 22:13 ssr_8797_R1_.merged.fastq\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, I might write: \n",
    "\n",
    "XXXXXXX\n",
    "\n",
    "Next, write down corresponding sample numbers, also separated by commas. MAKE SURE THEY CORRESPOND EXACTLY. For example:\n",
    "\n",
    "AB002,AB003,AB004\n",
    "\n",
    "The script we are going to use is called split_libraries_fastq.py. This script performs a number of functions, including demultiplexing of samples and quality filtering. Because fastq files provided by uBiome are already demultiplexed, we must pass this script the sample name associated with each file. For more information on qiime quality filtering, [check out this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3531572/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EDIT THIS CODE BEFORE YOU EVALUATE IT!\n",
    "!split_libraries_fastq.py -i [your list of fastq files] --sample_ids [your list of sample IDs] -o slout --barcode_type 'not-barcoded' -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!split_libraries_fastq.py -i ssr_8730_R1_.merged.fastq,ssr_8737_R1_.merged.fastq,ssr_8797_R1_.merged.fastq --sample_ids AB002,AB003,AB004 -o out --barcode_type 'not-barcoded' -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you (hopefully) have one file with all your mereged, cleaned, and filtered fastq reads in it. \n",
    "These reads should be identified with their sample numbers, for example:\n",
    "\n",
    "\\>AB002_0 NB501532:61:HGWJVAFXX:1:11101:9255:1102 1:N:0:CTACTCGA+CACATACG orig_bc=AAAAAAAAAAAA new_bc=AAAAAAAAAAAA bc_diffs=0\n",
    "CGTGTGCCAGCAGCCGCGGTAAGACGGGGGGGGCAAGTGTTCTTCGGAATGACTGGGCGTAAAGGGCACGTAGGCGGTGAATCGGGTTGAAAGTGAAAGTCGCCAAAAACTGGTGGAATGCTCTCGAAACCAATTCACTTGAGTGAGACAGTGGAATTTCGTGTGTAGGGGTGAAATCCGGAGATCTACGAAGGAAGGCCAAAAGCGAAGGCAGCTCTCTGGGTCCCCACCGACGCTGGAGTGCGAAAGCATGGGGAGCGAACGGGATTAGAAACCCCAGTAGTCCGGT\n",
    "\\>AB002_1 NB501532:61:HGWJVAFXX:1:11101:5822:1106 1:N:0:CTACTCGA+CACATACG orig_bc=AAAAAAAAAAAA new_bc=AAAAAAAAAAAA bc_diffs=0\n",
    "CGTGTGCCAGCAGCCGCGGTAATACAGAGGATGCAAGCGTTATCCGGAATGATTGGGCGTAAAGCGTCTGTAGGTGGCTTTTTAAGTCCGCCGTCAAATCCCAGGGCTCAACCCTGGACAGGCGGTGGAAACTACCAAGCTGGAGTACGGTCAGAGGGAATTTCCGGTGGAGCGGTGAAATGCGTAGAGATCGGAAAGAACACCAACGGCGAAAGCACTCTGCTGGGCCGACACTGACACTGAGAGACGAAAGCTAGGGGAGCGAATGGGATTAGAAACCCCAGTAGTCCGGA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##OTU Picking\n",
    "\n",
    "Next up, we'll be picking otu's! We will be using the script [pick_open_reference_otus.py](http://qiime.org/scripts/pick_open_reference_otus.html). We are specifying one important parameter, here, setting enable_rev_strand_match to True. This allows sequences to be considered a match if either their forward or reverse sequence hits the database. To set this parameter, we need to create a params file again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('uc_fast_params.txt', 'w+') as f:\n",
    "                        f.write('pick_otus:enable_rev_strand_match True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pick_open_reference_otus.py can take a looooong time depending on how big seqs.fna is. Around 3 hours for 15 merged samples, 1 hour for 3 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pick_open_reference_otus.py -o otus/ -i slout/seqs.fna -p uc_fast_params.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCRIPTION COPIED FROM [Illumina Overview Tutorial](http://nbviewer.jupyter.org/github/biocore/qiime/blob/1.9.1/examples/ipynb/illumina_overview_tutorial.ipynb)\n",
    "\n",
    "The primary output that we get from this command is the *OTU table*, or the number of times each operational taxonomic unit (OTU) is observed in each sample. QIIME uses the Genomics Standards Consortium Biological Observation Matrix standard (BIOM) format for representing OTU tables. You can find additional information on the BIOM format [here](http://www.biom-format.org), and information on converting these files to tab-separated text that can be viewed in spreadsheet programs [here](http://biom-format.org/documentation/biom_conversion.html). Several OTU tables are generated by this command. The one we typically want to work with is ``otus/otu_table_mc2_w_tax_no_pynast_failures.biom``. This has singleton OTUs (or OTUs with a total count of 1) removed, as well as OTUs whose representative (i.e., centroid) sequence couldn't be aligned with [PyNAST](http://bioinformatics.oxfordjournals.org/content/26/2/266.long). It also contains taxonomic assignments for each OTU as *observation metadata*.\n",
    "\n",
    "The open-reference OTU picking command also produces a phylogenetic tree where the tips are the OTUs. The file containing the tree is ``otus/rep_set.tre``, and is the file that should be used with ``otus/otu_table_mc2_w_tax_no_pynast_failures.biom`` in downstream phylogenetic diversity calculations. The tree is stored in the widely used [newick format](http://scikit-bio.org/docs/latest/generated/skbio.io.newick.html).\n",
    "\n",
    "To view the output of this command, call ``FileLink`` on the ``index.html`` file in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='otus/index.html' target='_blank'>otus/index.html</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/ubiomeSamples/merged/otus/index.html"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink('otus/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate some statistics on our OTUs using the biom program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples: 3\r\n",
      "Num observations: 13059\r\n",
      "Total count: 627933\r\n",
      "Table density (fraction of non-zero values): 0.378\r\n",
      "\r\n",
      "Counts/sample summary:\r\n",
      " Min: 103333.0\r\n",
      " Max: 279534.0\r\n",
      " Median: 245066.000\r\n",
      " Mean: 209311.000\r\n",
      " Std. dev.: 76247.462\r\n",
      " Sample Metadata Categories: None provided\r\n",
      " Observation Metadata Categories: taxonomy\r\n",
      "\r\n",
      "Counts/sample detail:\r\n",
      " AB002: 103333.0\r\n",
      " AB004: 245066.0\r\n",
      " AB003: 279534.0\r\n"
     ]
    }
   ],
   "source": [
    "!biom summarize-table -i otus/otu_table_mc2_w_tax_no_pynast_failures.biom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to pick what sequencing depth we would like to perform our rarefacation analysis on. Because different samples have different sequencing depths (i.e. numbers of reads) it would be wrong to assume that all had sequenced the diversity of the sample to the same extent. One common way to normalize sequencing depth across samples is to choose the sample with the lowest depth, and discard reads randomly in the remaining samples so that they all have the depth of the lowest sample. For a more detailed discussion on this, [see here](http://www.wernerlab.org/teaching/qiime/overview/e).\n",
    "\n",
    "Looking at the summary of your .biom file, note down the min number of reads in your samples (Min under Counts/sample summary). This is your sequencing depth. If you have one file that is far lower than all the others, it might make more sense to discard that sample from future analyses. You have to decide which is better: to discard one sample, or discard all the data from the other samples to acheive the same low sequencing depth as that one sample. \n",
    "\n",
    "##Core Diversity Analysis\n",
    "\n",
    "The script core_diversity_analyses.py will give us a first glance look at some important information about our samples. At this point, we need our mapping file. \n",
    "\n",
    "-e specifies the sequencing depth you will be using. The one I have is based on my data, replace it with something appropriate for yours! This step can also take a long time (~30 minutes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ubiomeSamples/merged\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/bin/core_diversity_analyses.py\", line 202, in <module>\r\n",
      "    main()\r\n",
      "  File \"/usr/local/bin/core_diversity_analyses.py\", line 199, in main\r\n",
      "    status_update_callback=status_update_callback)\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/qiime/workflow/core_diversity_analyses.py\", line 252, in run_core_diversity_analyses\r\n",
      "    status_update_callback=status_update_callback)\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/qiime/workflow/downstream.py\", line 183, in run_beta_diversity_through_plots\r\n",
      "    close_logger_on_success=close_logger_on_success)\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/qiime/workflow/util.py\", line 122, in call_commands_serially\r\n",
      "    raise WorkflowError(msg)\r\n",
      "qiime.workflow.util.WorkflowError: \r\n",
      "\r\n",
      "*** ERROR RAISED DURING STEP: Make emperor plots, weighted_unifrac)\r\n",
      "Command run was:\r\n",
      " make_emperor.py -i cdout//bdiv_even103333//weighted_unifrac_pc.txt -o cdout//bdiv_even103333//weighted_unifrac_emperor_pcoa_plot/ -m ../map.tsv \r\n",
      "Command returned exit status: 2\r\n",
      "Stdout:\r\n",
      "\r\n",
      "Stderr\r\n",
      "Error in make_emperor.py: Due to the variation explained, Emperor could not plot at least 3 axes, check the input files to ensure that the percent explained is greater than 0.01 in at least three axes.\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!core_diversity_analyses.py -o cdout/ -i otus/otu_table_mc2_w_tax_no_pynast_failures.biom -m ../map.tsv -t otus/rep_set.tre -e 103333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ubiomeSamples/merged\r\n"
     ]
    }
   ],
   "source": [
    "FileLink('cdout/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You are done. You can look through the outputs using the file link above. \n",
    "\n",
    "Next, we need to transfer these outputs back to our local computer, for analysis with RDP.\n",
    "\n",
    "To transfer files, use scp from a local terminal window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scp -r -i [location of your key] ubuntu@[public DNS]:/home/ubuntu/ubiomeSamples/merged/ [location you want to save the files to]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You are all done with the community AMI. You may stop it, or terminate it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
